The aim is just to show how to build the simplest Long short-term memory (LSTM) recurrent neural network for the data.
The description of data can be found here:
http://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption

Attribute Information:
1.date: Date in format dd/mm/yyyy
2.time: time in format hh:mm:ss
3.global_active_power: household global minute-averaged active power (in kilowatt)
4.global_reactive_power: household global minute-averaged reactive power (in kilowatt)
5.voltage: minute-averaged voltage (in volt)
6.global_intensity: household global minute-averaged current intensity (in ampere)
7.sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).
8.sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.
9.sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.
In [2]:
# Let`s import all packages that we may need:

import sys 
import numpy as np # linear algebra
from scipy.stats import randint
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL
import matplotlib.pyplot as plt # this is used for the plot the graph 
import seaborn as sns # used for plot interactive graph. 
from sklearn.model_selection import train_test_split # to split the data into two parts
from sklearn.cross_validation import KFold # use for cross validation
from sklearn.preprocessing import StandardScaler # for normalization
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline # pipeline making
from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import SelectFromModel
from sklearn import metrics # for the check the error and accuracy of the model
from sklearn.metrics import mean_squared_error,r2_score

## for Deep-learing:
import keras
from keras.layers import Dense
from keras.models import Sequential
from keras.utils import to_categorical
from keras.optimizers import SGD 
from keras.callbacks import EarlyStopping
from keras.utils import np_utils
import itertools
from keras.layers import LSTM
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers import Dropout
Importing the data and data processing
In [3]:
## Data can be downloaded from: http://archive.ics.uci.edu/ml/machine-learning-databases/00235/
## Just open the zip file and grab the file 'household_power_consumption.txt' put it in the directory 
## that you would like to run the code. 

df = pd.read_csv('household_power_consumption.txt', sep=';', 
                 parse_dates={'dt' : ['Date', 'Time']}, infer_datetime_format=True, 
                 low_memory=False, na_values=['nan','?'], index_col='dt')
1) Note that data include 'nan' and '?' as a string. I converted both to numpy nan in importing stage (above) and treated both of them the same.
2) I merged two columns 'Date' and 'Time' to 'dt'.
3) I also converted in the above, the data to time-series type, by taking index to be the time.
In [5]:
df.head()
Out[5]:
Global_active_power	Global_reactive_power	Voltage	Global_intensity	Sub_metering_1	Sub_metering_2	Sub_metering_3
dt							
2006-12-16 17:24:00	4.216	0.418	234.84	18.4	0.0	1.0	17.0
2006-12-16 17:25:00	5.360	0.436	233.63	23.0	0.0	1.0	16.0
2006-12-16 17:26:00	5.374	0.498	233.29	23.0	0.0	2.0	17.0
2006-12-16 17:27:00	5.388	0.502	233.74	23.0	0.0	1.0	17.0
2006-12-16 17:28:00	3.666	0.528	235.68	15.8	0.0	1.0	17.0
In [6]:
df.info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 2075259 entries, 2006-12-16 17:24:00 to 2010-11-26 21:02:00
Data columns (total 7 columns):
Global_active_power      float64
Global_reactive_power    float64
Voltage                  float64
Global_intensity         float64
Sub_metering_1           float64
Sub_metering_2           float64
Sub_metering_3           float64
dtypes: float64(7)
memory usage: 126.7 MB
In [113]:
df.dtypes
Out[113]:
Global_active_power      float64
Global_reactive_power    float64
Voltage                  float64
Global_intensity         float64
Sub_metering_1           float64
Sub_metering_2           float64
Sub_metering_3           float64
dtype: object
In [7]:
df.shape
Out[7]:
(2075259, 7)
In [115]:
df.describe()
Out[115]:
Global_active_power	Global_reactive_power	Voltage	Global_intensity	Sub_metering_1	Sub_metering_2	Sub_metering_3
count	2.049280e+06	2.049280e+06	2.049280e+06	2.049280e+06	2.049280e+06	2.049280e+06	2.049280e+06
mean	1.091615e+00	1.237145e-01	2.408399e+02	4.627759e+00	1.121923e+00	1.298520e+00	6.458447e+00
std	1.057294e+00	1.127220e-01	3.239987e+00	4.444396e+00	6.153031e+00	5.822026e+00	8.437154e+00
min	7.600000e-02	0.000000e+00	2.232000e+02	2.000000e-01	0.000000e+00	0.000000e+00	0.000000e+00
25%	3.080000e-01	4.800000e-02	2.389900e+02	1.400000e+00	0.000000e+00	0.000000e+00	0.000000e+00
50%	6.020000e-01	1.000000e-01	2.410100e+02	2.600000e+00	0.000000e+00	0.000000e+00	1.000000e+00
75%	1.528000e+00	1.940000e-01	2.428900e+02	6.400000e+00	0.000000e+00	1.000000e+00	1.700000e+01
max	1.112200e+01	1.390000e+00	2.541500e+02	4.840000e+01	8.800000e+01	8.000000e+01	3.100000e+01
In [8]:
df.columns
Out[8]:
Index(['Global_active_power', 'Global_reactive_power', 'Voltage',
       'Global_intensity', 'Sub_metering_1', 'Sub_metering_2',
       'Sub_metering_3'],
      dtype='object')
In [9]:
for j in range(1,7):
       print(df.iloc[:, j].unique())
[ 0.418  0.436  0.498  0.502  0.528  0.522  0.52   0.51   0.47   0.478
  0.398  0.422  0.282  0.152  0.156  0.     0.076  0.09   0.2    0.058
  0.18   0.144  0.118  0.108  0.202  0.192  0.186  0.116  0.136  0.148
  0.16   0.158  0.1    0.082  0.05   0.052  0.162  0.086  0.048  0.054
  0.068  0.166  0.174  0.178  0.188  0.088  0.08   0.07   0.084  0.074
  0.106  0.092  0.064  0.046  0.138  0.134  0.132  0.12   0.13   0.154
  0.078  0.142  0.14   0.146  0.15   0.17   0.102  0.072  0.06   0.056
  0.062  0.112  0.066  0.172  0.168  0.194  0.184  0.096  0.164  0.182
  0.094  0.098  0.176  0.19   0.204  0.22   0.198  0.208  0.244  0.334
  0.298  0.296  0.286  0.278  0.104  0.258  0.238  0.256  0.214  0.23   0.21
  0.24   0.124  0.122  0.27   0.302  0.272  0.25   0.294  0.3    0.128
  0.126  0.234  0.242  0.316  0.28   0.288  0.224  0.11   0.248  0.254
  0.222  0.246  0.216  0.212  0.114  0.206  0.228  0.226  0.196  0.218
  0.368  0.338  0.31   0.304  0.292  0.268  0.266  0.26   0.396  0.38   0.4
  0.384  0.416  0.404  0.382  0.232  0.314  0.312  0.262  0.252  0.236
  0.348  0.342  0.34   0.35   0.504  0.344  0.346  0.366  0.574  0.582
  0.37   0.364  0.36   0.362  0.372  0.392  0.354  0.356  0.358  0.402
  0.394  0.39   0.386  0.274  0.264  0.324  0.352  0.276  0.306  0.376
  0.562  0.44   0.378  0.426  0.45   0.374  0.388  0.284  0.29   0.308
  0.318  0.32   0.322  0.326  0.336  0.414  0.408  0.472  0.464  0.462
  0.578  0.576  0.58   0.474  0.514  0.646  0.636  0.598  0.586  0.566
  0.406  0.43   0.412  0.442  0.452  0.432  0.33   0.328  0.538  0.564
  0.552  0.556  0.606  0.424  0.492  0.526  0.512  0.482  0.446  0.332
    nan  0.548  0.534  0.532  0.688  0.664  0.66   0.53   0.516  0.572
  0.54   0.614  0.568  0.444  0.456  0.448  0.48   0.434  0.438  0.494
  0.496  0.468  0.56   0.622  0.57   0.618  0.624  0.466  0.42   0.5    0.602
  0.632  0.724  0.706  0.484  0.458  0.428  0.486  0.698  0.708  0.686
  0.666  0.658  0.49   0.454  0.64   0.59   0.524  0.634  0.476  0.694  0.7
  0.794  0.592  0.46   0.656  0.644  0.488  0.41   0.55   0.616  0.8    0.79
  0.788  0.67   0.506  0.6    0.68   0.678  0.682  0.69   0.508  0.604
  0.608  0.554  0.518  0.65   0.648  0.642  0.654  0.754  0.75   0.742
  0.594  0.536  0.544  0.63   0.542  0.672  0.588  0.596  0.584  0.674
  0.726  0.768  0.774  0.712  0.856  0.862  0.848  0.73   0.71   0.662
  0.722  0.704  0.546  0.638  0.628  0.62   0.716  0.778  0.702  0.676
  0.834  0.802  0.772  0.558  0.626  0.652  0.76   0.78   0.784  0.786
  0.792  0.782  0.832  0.842  0.744  0.822  0.612  0.692  0.684  0.874
  0.804  0.696  0.61   0.718  0.732  0.72   0.748  0.796  0.736  0.82
  0.808  0.756  0.776  0.806  0.764  0.818  0.77   0.758  0.858  0.86   0.85
  0.752  0.714  0.84   0.83   0.838  0.738  0.828  0.74   0.746  0.988
  0.966  0.96   0.956  0.854  0.912  0.734  0.728  0.826  0.88   0.878
  0.876  0.93   0.986  0.972  0.946  0.902  1.046  1.148  1.054  0.89
  0.668  1.092  0.996  0.974  1.04   1.06   1.028  0.908  0.898  0.95
  0.964  0.94   0.928  0.896  0.868  0.87   0.866  0.864  0.872  0.812
  0.836  0.824  0.814  0.762  0.994  1.032  0.816  0.944  0.968  0.916
  1.014  1.008  1.002  0.81   1.     1.006  1.012  1.034  0.852  0.924
  0.942  0.982  0.906  0.932  0.914  0.846  0.844  1.222  1.19   1.18
  1.118  1.082  1.08   0.91   1.102  1.274  1.364  1.39   0.918  1.198
  1.192  0.936  0.954  0.98   0.99   0.904  0.798  0.766  1.09   1.016
  0.922  1.036  0.892  0.894  0.9    0.886  1.096  0.92   0.962  0.958
  0.998  1.044  1.048  0.938  0.934  1.13   1.218  0.888  0.882  1.156
  1.024  1.088  1.084  1.076  1.066  1.07   1.24   0.926  0.952  0.884
  1.186  1.108  1.03   1.02   0.978  0.97   0.948  1.064  1.078  1.074
  1.124  1.018  0.976  1.038  1.094  1.1    1.026]
[ 234.84  233.63  233.29 ...,  251.63  253.08  253.16]
[ 18.4  23.   15.8  15.   19.6  23.2  22.4  22.6  17.6  14.2  13.8  14.4
  16.4  25.4  33.2  30.6  22.   19.4  13.6  13.4  11.6  19.2  10.4  11.8
  11.   21.   26.2  29.   27.8  27.   19.    9.6  10.   21.4  17.8  12.
  12.4  15.2  20.8  15.6  26.4  18.8  13.    9.8   9.2  13.2  18.   10.8
  10.6  18.2  17.   17.4  14.8  14.6  15.4  14.    8.4   9.4   7.8   8.2
   8.6  11.4  12.6   9.    7.6  12.8   7.4   5.6   4.6   7.2   1.6   2.4
   2.8   1.2   4.4   4.   10.2   6.8   5.2   6.    6.6   8.    2.    4.8
   5.4  16.6   3.    1.8   8.8   1.4  21.8  20.2  16.   18.6  16.8  11.2
   2.6  16.2   4.2   7.    3.8   3.2   5.    3.6   0.8   5.8   1.    6.4
   6.2   3.4   2.2  12.2  30.   29.4  28.2  25.8  20.   24.8  24.6  25.2
  25.   23.4  23.8  20.6  21.6  19.8  20.4  26.6  17.2  21.2  23.6  29.2
  33.6  28.   30.2  33.4  24.2  24.   22.2  24.4  22.8   nan  30.4  34.2
  32.8  33.   32.6  31.6  28.6  25.6  31.   28.4  27.2  29.8  29.6  37.
  36.8  32.4  27.4  35.   34.8  32.2  26.   35.2  36.2  27.6  26.8  34.4
  31.4  30.8  31.2  39.   39.2  39.4  28.8  38.8  35.6  32.   33.8  36.
  37.2  34.   31.8  36.6  40.4  37.4  38.2  36.4  35.4  34.6  35.8  38.
  40.2  41.   41.4  41.2  41.8  38.6  44.4  46.4  43.2  39.8  40.8   0.6
   0.4  37.8  40.6  38.4  42.   42.2  39.6  42.6  44.2  37.6  40.    0.2
  44.6  43.4  43.   45.8  48.4  42.8]
[  0.   1.   2.   6.  37.  36.  38.  25.  12.  11.   9.   4.  33.   5.  34.
   8.  26.  27.  15.  20.  21.  23.  16.  10.  39.  32.  nan  19.  35.  30.
  40.   7.  31.  17.   3.  29.  24.  14.  41.  53.  76.  75.  77.  74.  28.
  68.  58.  62.  13.  18.  22.  64.  71.  56.  72.  45.  42.  59.  66.  43.
  44.  48.  46.  73.  55.  60.  70.  63.  67.  47.  65.  51.  50.  69.  78.
  57.  49.  61.  52.  54.  79.  80.  81.  82.  83.  84.  88.  86.  87.]
[  1.   2.   0.  16.  37.  36.  22.  12.  27.  20.   9.   4.   3.   5.  13.
  35.  28.  26.  24.  21.  17.  33.  25.  23.  18.  30.  31.   7.  38.  29.
  50.  71.  59.  62.  70.  40.  34.   8.  15.   6.  32.  19.  14.  49.  72.
  73.  61.  39.  42.  10.  56.  11.  nan  43.  67.  63.  53.  65.  52.  69.
  48.  74.  41.  47.  58.  68.  64.  75.  46.  57.  60.  66.  44.  78.  77.
  76.  51.  55.  45.  54.  79.  80.]
[ 17.  16.  18.   9.   0.   5.  19.  10.  20.   8.   4.   2.   3.  11.  13.
   6.  15.  nan  12.   1.  14.   7.  23.  25.  26.  28.  27.  21.  22.  29.
  24.  30.  31.]
Dealing with missing values 'nan' with a test statistic
In [10]:
## finding all columns that have nan:

droping_list_all=[]
for j in range(0,7):
    if not df.iloc[:, j].notnull().all():
        droping_list_all.append(j)        
        #print(df.iloc[:,j].unique())
droping_list_all
Out[10]:
[0, 1, 2, 3, 4, 5, 6]
In [11]:
# filling nan with mean in any columns

for j in range(0,7):        
        df.iloc[:,j]=df.iloc[:,j].fillna(df.iloc[:,j].mean())
In [12]:
# another sanity check to make sure that there are not more any nan
df.isnull().sum()
Out[12]:
Global_active_power      0
Global_reactive_power    0
Voltage                  0
Global_intensity         0
Sub_metering_1           0
Sub_metering_2           0
Sub_metering_3           0
dtype: int64
In [13]:
df.describe()
Out[13]:
Global_active_power	Global_reactive_power	Voltage	Global_intensity	Sub_metering_1	Sub_metering_2	Sub_metering_3
count	2.075259e+06	2.075259e+06	2.075259e+06	2.075259e+06	2.075259e+06	2.075259e+06	2.075259e+06
mean	1.091615e+00	1.237145e-01	2.408399e+02	4.627759e+00	1.121923e+00	1.298520e+00	6.458447e+00
std	1.050655e+00	1.120142e-01	3.219643e+00	4.416490e+00	6.114397e+00	5.785470e+00	8.384178e+00
min	7.600000e-02	0.000000e+00	2.232000e+02	2.000000e-01	0.000000e+00	0.000000e+00	0.000000e+00
25%	3.100000e-01	4.800000e-02	2.390200e+02	1.400000e+00	0.000000e+00	0.000000e+00	0.000000e+00
50%	6.300000e-01	1.020000e-01	2.409600e+02	2.800000e+00	0.000000e+00	0.000000e+00	1.000000e+00
75%	1.520000e+00	1.920000e-01	2.428600e+02	6.400000e+00	0.000000e+00	1.000000e+00	1.700000e+01
max	1.112200e+01	1.390000e+00	2.541500e+02	4.840000e+01	8.800000e+01	8.000000e+01	3.100000e+01
In [14]:
df['Global_active_power'].resample('M').sum()
Out[14]:
dt
2006-12-31    41817.648460
2007-01-31    69014.045230
2007-02-28    56491.069230
2007-03-31    58863.283615
2007-04-30    39245.548781
2007-05-31    44008.872000
2007-06-30    35729.767447
2007-07-31    29846.831570
2007-08-31    34120.475531
2007-09-30    41874.789230
2007-10-31    49278.553230
2007-11-30    55920.827230
2007-12-31    72605.261615
2008-01-31    65170.473615
2008-02-29    49334.346845
2008-03-31    55591.685615
2008-04-30    48209.992000
2008-05-31    45724.043230
2008-06-30    42945.063615
2008-07-31    35479.601230
2008-08-31    12344.063230
2008-09-30    42667.792000
2008-10-31    50743.399447
2008-11-30    59918.584535
2008-12-31    56911.416668
2009-01-31    62951.099615
2009-02-28    50291.953362
2009-03-31    54761.169230
2009-04-30    49277.707230
2009-05-31    45214.196460
2009-06-30    37149.767696
2009-07-31    27594.810460
2009-08-31    30049.032998
2009-09-30    42631.838845
2009-10-31    51089.811615
2009-11-30    55068.733615
2009-12-31    60907.189230
2010-01-31    62797.504679
2010-02-28    55473.889230
2010-03-31    50368.601679
2010-04-30    44379.215615
2010-05-31    48893.491615
2010-06-30    41887.607230
2010-07-31    32188.843615
2010-08-31    29991.384254
2010-09-30    42026.211946
2010-10-31    51934.045615
2010-11-30    44598.388000
Freq: M, Name: Global_active_power, dtype: float64
Data visualization
* Below I resample over day, and show the sum and mean of Global_active_power. It is seen that mean and sum of resampled data set, have similar structure.
In [15]:
df.Global_active_power.resample('D').sum().plot(title='Global_active_power resampled over day for sum') 
#df.Global_active_power.resample('D').mean().plot(title='Global_active_power resampled over day', color='red') 
plt.tight_layout()
plt.show()   

df.Global_active_power.resample('D').mean().plot(title='Global_active_power resampled over day for mean', color='red') 
plt.tight_layout()
plt.show()


Below I show mean and std of 'Global_intensity' resampled over day
In [194]:
r = df.Global_intensity.resample('D').agg(['mean', 'std'])
r.plot(subplots = True, title='Global_intensity resampled over day')
plt.show()

Below I show mean and std of 'Global_reactive_power' resampled over day
In [204]:
r2 = df.Global_reactive_power.resample('D').agg(['mean', 'std'])
r2.plot(subplots = True, title='Global_reactive_power resampled over day', color='red')
plt.show()

Sum of 'Global_active_power' resampled over month
In [286]:
# Sum of 'Global_active_power' resampled over month
df['Global_active_power'].resample('M').mean().plot(kind='bar')
plt.xticks(rotation=60)
plt.ylabel('Global_active_power')
plt.title('Global_active_power per month (averaged over month)')
plt.show()

Mean of 'Global_active_power' resampled over quarter
In [287]:
df['Global_active_power'].resample('Q').mean().plot(kind='bar')
plt.xticks(rotation=60)
plt.ylabel('Global_active_power')
plt.title('Global_active_power per quarter (averaged over quarter)')
plt.show()

* It is very important to note from above two plots that resampling over larger time inteval, will diminish the periodicity of system as we expect. This is important for machine learning feature engineering.
mean of 'Voltage' resampled over month
In [291]:
df['Voltage'].resample('M').mean().plot(kind='bar', color='red')
plt.xticks(rotation=60)
plt.ylabel('Voltage')
plt.title('Voltage per quarter (summed over quarter)')
plt.show()

In [292]:
df['Sub_metering_1'].resample('M').mean().plot(kind='bar', color='brown')
plt.xticks(rotation=60)
plt.ylabel('Sub_metering_1')
plt.title('Sub_metering_1 per quarter (summed over quarter)')
plt.show()

* It is seen from the above plots that the mean of 'Volage' over month is pretty much constant compared to other features. This is important again in feature selection.
Below I compare the mean of different features resampled over day.
In [294]:
# Below I compare the mean of different featuresresampled over day. 
# specify columns to plot
cols = [0, 1, 2, 3, 5, 6]
i = 1
values = df.resample('D').mean().values
# plot each column
plt.figure(figsize=(15, 10))
for group in groups:
	plt.subplot(len(cols), 1, i)
	plt.plot(values[:, group])
	plt.title(df.columns[group], y=0.75, loc='right')
	i += 1
plt.show()

In [180]:
## resampling over week and computing mean
df.Global_reactive_power.resample('W').mean().plot(color='y', legend=True)
df.Global_active_power.resample('W').mean().plot(color='r', legend=True)
df.Sub_metering_1.resample('W').mean().plot(color='b', legend=True)
df.Global_intensity.resample('W').mean().plot(color='g', legend=True)
plt.show()

In [295]:
# Below I show hist plot of the mean of different feature resampled over month 
df.Global_active_power.resample('M').mean().plot(kind='hist', color='r', legend=True )
df.Global_reactive_power.resample('M').mean().plot(kind='hist',color='b', legend=True)
#df.Voltage.resample('M').sum().plot(kind='hist',color='g', legend=True)
df.Global_intensity.resample('M').mean().plot(kind='hist', color='g', legend=True)
df.Sub_metering_1.resample('M').mean().plot(kind='hist', color='y', legend=True)
plt.show()

The correlations between 'Global_intensity' and 'Global_active_power'
In [199]:
## The correlations between 'Global_intensity', 'Global_active_power'
data_returns = df.pct_change()
sns.jointplot(x='Global_intensity', y='Global_active_power', data=data_returns)  

plt.show()

In [200]:
## The correlations between 'Voltage' and  'Global_active_power'
sns.jointplot(x='Voltage', y='Global_active_power', data=data_returns)  
plt.show()

* From above two plots it is seen that 'Global_intensity' and 'Global_active_power' correlated. But 'Voltage', 'Global_active_power' are less correlated. This is important observation for machine learning purpose.
Correlations among features
In [303]:
# Correlations among columns
plt.matshow(df.corr(method='spearman'),vmax=1,vmin=-1,cmap='PRGn')
plt.title('without resampling', size=15)
plt.colorbar()
plt.show()

Correlations of mean of resampled features
In [16]:
# Correlations of mean of features resampled over months


plt.matshow(df.resample('M').mean().corr(method='spearman'),vmax=1,vmin=-1,cmap='PRGn')
plt.title('resampled over month', size=15)
plt.colorbar()
plt.margins(0.02)
plt.matshow(df.resample('A').mean().corr(method='spearman'),vmax=1,vmin=-1,cmap='PRGn')
plt.title('resampled over year', size=15)
plt.colorbar()
plt.show()


* It is seen from above that with resampling techniques one can change the correlations among features. This is important for feature engineering.
Machine-Leaning: LSTM Data Preparation and feature engineering
* I will apply recurrent nueral network (LSTM) which is best suited for time-seriers and sequential problem. This approach is the best if we have large data.
 * I will frame the supervised learning problem as predicting the Global_active_power at the current time (t) given the Global_active_power measurement and other features at the prior time step.
In [17]:
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
	n_vars = 1 if type(data) is list else data.shape[1]
	dff = pd.DataFrame(data)
	cols, names = list(), list()
	# input sequence (t-n, ... t-1)
	for i in range(n_in, 0, -1):
		cols.append(dff.shift(i))
		names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
	# forecast sequence (t, t+1, ... t+n)
	for i in range(0, n_out):
		cols.append(dff.shift(-i))
		if i == 0:
			names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
		else:
			names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
	# put it all together
	agg = pd.concat(cols, axis=1)
	agg.columns = names
	# drop rows with NaN values
	if dropnan:
		agg.dropna(inplace=True)
	return agg
* In order to reduce the computation time, and also get a quick result to test the model. One can resmaple the data over hour (the original data are given in minutes). This will reduce the size of data from 2075259 to 34589 but keep the overall strucure of data as shown in the above.
In [54]:
## resampling of data over hour
df_resample = df.resample('h').mean() 
df_resample.shape
Out[54]:
(34589, 7)
* Note: I scale all features in range of [0,1].
In [60]:
## If you would like to train based on the resampled data (over hour), then used below
values = df_resample.values 


## full data without resampling
#values = df.values

# integer encode direction
# ensure all data is float
#values = values.astype('float32')
# normalize features
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(values)
# frame as supervised learning
reframed = series_to_supervised(scaled, 1, 1)

# drop columns we don't want to predict
reframed.drop(reframed.columns[[8,9,10,11,12,13]], axis=1, inplace=True)
print(reframed.head())
   var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var5(t-1)  var6(t-1)  \
1   0.636816   0.295738   0.337945   0.631157        0.0   0.011366   
2   0.545045   0.103358   0.335501   0.541487        0.0   0.144652   
3   0.509006   0.110073   0.283802   0.502152        0.0   0.030869   
4   0.488550   0.096987   0.315987   0.481110        0.0   0.000000   
5   0.455597   0.099010   0.434417   0.449904        0.0   0.008973   

   var7(t-1)   var1(t)  
1   0.782418  0.545045  
2   0.782676  0.509006  
3   0.774169  0.488550  
4   0.778809  0.455597  
5   0.798917  0.322555  
* Above I showed 7 input variables (input series) and the 1 output variable for 'Global_active_power' at the current time in hour (depending on resampling).
Splitting the rest of data to train and validation sets
* First, I split the prepared dataset into train and test sets. To speed up the training of the model (for the sake of the demonstration), we will only train the model on the first year of data, then evaluate it on the next 3 years of data.
In [63]:
# split into train and test sets
values = reframed.values

n_train_time = 365*24
train = values[:n_train_time, :]
test = values[n_train_time:, :]
##test = values[n_train_time:n_test_time, :]
# split into input and outputs
train_X, train_y = train[:, :-1], train[:, -1]
test_X, test_y = test[:, :-1], test[:, -1]
# reshape input to be 3D [samples, timesteps, features]
train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
print(train_X.shape, train_y.shape, test_X.shape, test_y.shape) 
# We reshaped the input into the 3D format as expected by LSTMs, namely [samples, timesteps, features].
(8760, 1, 7) (8760,) (25828, 1, 7) (25828,)
Model architecture
1) LSTM with 100 neurons in the first visible layer
3) dropout 20%
4) 1 neuron in the output layer for predicting Global_active_power.
5) The input shape will be 1 time step with 7 features.
6) I use the Mean Absolute Error (MAE) loss function and the efficient Adam version of stochastic gradient descent.
7) The model will be fit for 20 training epochs with a batch size of 70.
In [62]:
model = Sequential()
model.add(LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(Dropout(0.2))
#    model.add(LSTM(70))
#    model.add(Dropout(0.3))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')



# fit network
history = model.fit(train_X, train_y, epochs=20, batch_size=70, validation_data=(test_X, test_y), verbose=2, shuffle=False)

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

# make a prediction
yhat = model.predict(test_X)
test_X = test_X.reshape((test_X.shape[0], 7))
# invert scaling for forecast
inv_yhat = np.concatenate((yhat, test_X[:, -6:]), axis=1)
inv_yhat = scaler.inverse_transform(inv_yhat)
inv_yhat = inv_yhat[:,0]
# invert scaling for actual
test_y = test_y.reshape((len(test_y), 1))
inv_y = np.concatenate((test_y, test_X[:, -6:]), axis=1)
inv_y = scaler.inverse_transform(inv_y)
inv_y = inv_y[:,0]
# calculate RMSE
rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))
print('Test RMSE: %.3f' % rmse)
Train on 8760 samples, validate on 25828 samples
Epoch 1/20
3s - loss: 0.0193 - val_loss: 0.0123
Epoch 2/20
2s - loss: 0.0128 - val_loss: 0.0109
Epoch 3/20
2s - loss: 0.0115 - val_loss: 0.0098
Epoch 4/20
2s - loss: 0.0109 - val_loss: 0.0095
Epoch 5/20
2s - loss: 0.0106 - val_loss: 0.0093
Epoch 6/20
2s - loss: 0.0106 - val_loss: 0.0092
Epoch 7/20
2s - loss: 0.0106 - val_loss: 0.0093
Epoch 8/20
2s - loss: 0.0105 - val_loss: 0.0093
Epoch 9/20
2s - loss: 0.0105 - val_loss: 0.0093
Epoch 10/20
2s - loss: 0.0104 - val_loss: 0.0093
Epoch 11/20
1s - loss: 0.0105 - val_loss: 0.0093
Epoch 12/20
2s - loss: 0.0104 - val_loss: 0.0092
Epoch 13/20
2s - loss: 0.0104 - val_loss: 0.0093
Epoch 14/20
2s - loss: 0.0104 - val_loss: 0.0092
Epoch 15/20
2s - loss: 0.0105 - val_loss: 0.0093
Epoch 16/20
2s - loss: 0.0104 - val_loss: 0.0093
Epoch 17/20
2s - loss: 0.0104 - val_loss: 0.0093
Epoch 18/20
2s - loss: 0.0104 - val_loss: 0.0092
Epoch 19/20
2s - loss: 0.0104 - val_loss: 0.0093
Epoch 20/20
2s - loss: 0.0104 - val_loss: 0.0092

Test RMSE: 0.618
 Note that in order to improve the model, one has to adjust epochs and batch_size.
Final remarks
* Here I have used the LSTM which is now the state-of-the-art for sequencial or time-series problems.
* In order to reduce the computation time, and get some results quickly, I took the first year of data (resampled over hour) to train the model and the rest of data to test the model. The above codes work for any time interval (just one has to change one line to change the interval).¶
* I put together a very simple LSTM neural-network to show that one can obtain reasonable predictions. However numbers of rows is too high and as a result the computation is very time-consuming (even for the simple model in the above it took few mins to be run on 2.8 GHz Intel Core i7). The Best is to write the last part of code using Spark (MLlib) running on GPU.
* Moreover, the neural-network architecture that I have designed is a toy model. It can be easily improved by dropout and adding CNN layers. The CNN is useful here since there are correlations in data (CNN layer is a good way to probe the local structure of data).
